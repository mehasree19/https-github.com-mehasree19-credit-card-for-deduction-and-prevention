# -*- coding: utf-8 -*-
"""Meha NM ASSIGNMENT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mAC33TXnCMi5YFiPmBoMENaWg9z_8yI5

## **AI-Powered Credit Card Fraud Detection:**

## Data loading

### Subtask:
Load the credit card transaction data from the provided CSV file.

**Reasoning**:
Load the credit card transaction data from the provided CSV file and display the first few rows.
"""

import pandas as pd

try:
    df = pd.read_csv('creditcard.csv')
    display(df.head())
except FileNotFoundError:
    print("Error: 'creditcard.csv' not found. Please ensure the file is in the correct location.")
    df = None  # Indicate that the DataFrame was not created
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    df = None

"""## Data exploration

### Subtask:
Explore the loaded credit card transaction data to understand its characteristics.

**Reasoning**:
Explore the data by displaying its shape, data types, target variable distribution, missing values, and feature distributions.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Data Overview
print("Shape of the DataFrame:", df.shape)
print("\nData types of each column:")
print(df.info())

# 2. Target Variable Identification
print("\nDistribution of the target variable 'Class':")
print(df['Class'].value_counts(normalize=True) * 100)
plt.figure(figsize=(6, 4))
sns.countplot(x='Class', data=df)
plt.title('Distribution of Transaction Class')
plt.show()


# 3. Missing Value Check
print("\nMissing values per column:")
print(df.isnull().sum())

# 4. Feature Distribution Analysis
numerical_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12',
                     'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23',
                     'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Time']

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features):
  plt.subplot(6, 5, i + 1)
  sns.histplot(df[col], kde=True)
  plt.title(col)
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features):
    plt.subplot(6, 5, i + 1)
    sns.boxplot(x='Class', y=col, data=df)
    plt.title(f'{col} vs Class')
plt.tight_layout()
plt.show()

# Correlation analysis (commented out to avoid excessive output)
# print("\nCorrelation between numerical features and the target variable:")
# print(df[numerical_features + ['Class']].corr()['Class'])

"""## Data preparation

### Subtask:
Prepare the data for model training by handling class imbalance and scaling numerical features.

**Reasoning**:
Handle class imbalance using random oversampling and scale numerical features using standardization.
"""

from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Handle class imbalance using random oversampling
oversampler = RandomOverSampler(random_state=42)

# Drop rows with NaN values in 'Class' column before oversampling
df = df.dropna(subset=['Class'])  # Drop rows with NaN in 'Class'

X = df.drop('Class', axis=1)
y = df['Class']
X_resampled, y_resampled = oversampler.fit_resample(X, y)
df_balanced = pd.DataFrame(X_resampled, columns=X.columns)
df_balanced['Class'] = y_resampled

# Scale numerical features using standardization
scaler = StandardScaler()
numerical_features = ['V' + str(i) for i in range(1, 29)] + ['Amount']
df_balanced[numerical_features] = scaler.fit_transform(df_balanced[numerical_features])
df_scaled = df_balanced

display(df_scaled.head())

"""## Data splitting

### Subtask:
Split the prepared dataset into training, testing, and validation sets.

**Reasoning**:
Split the scaled dataset into training, testing, and validation sets using train_test_split.
"""

from sklearn.model_selection import train_test_split

# Split data into training and temporary sets (test + validation)
X = df_scaled.drop('Class', axis=1)
y = df_scaled['Class']
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Split temporary set into testing and validation sets
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Verify shapes
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)

"""## Feature engineering

### Subtask:
Explore and engineer new features from the existing dataset to potentially improve model performance.

**Reasoning**:
Create new features based on the analysis of existing features, especially 'Time' and 'Amount', and evaluate their impact on model performance using a simple model.
"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, f1_score

# Feature Engineering
df_scaled['Amount_log'] = np.log1p(df_scaled['Amount'])  # Log transformation of Amount
df_scaled['Time_hour'] = (df_scaled['Time'] / 3600) % 24  # Hour of the day

# Interaction features
for i in range(1, 29):
    df_scaled[f'Amount_x_V{i}'] = df_scaled['Amount'] * df_scaled[f'V{i}']

# Polynomial features
df_scaled['Amount_squared'] = df_scaled['Amount']**2

# Prepare data
X = df_scaled.drop('Class', axis=1)
y = df_scaled['Class']
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Train and evaluate a Logistic Regression model
model = LogisticRegression(max_iter=1000, random_state=42)

# Evaluate with original features
model.fit(X_train.drop(['Amount_log', 'Time_hour', 'Amount_squared'] + [f'Amount_x_V{i}' for i in range(1, 29)], axis=1), y_train)
y_pred_original = model.predict_proba(X_val.drop(['Amount_log', 'Time_hour', 'Amount_squared'] + [f'Amount_x_V{i}' for i in range(1, 29)], axis=1))[:, 1]
auc_original = roc_auc_score(y_val, y_pred_original)
f1_original = f1_score(y_val, (y_pred_original > 0.5).astype(int))

# Evaluate with engineered features
model.fit(X_train, y_train)
y_pred_engineered = model.predict_proba(X_val)[:, 1]
auc_engineered = roc_auc_score(y_val, y_pred_engineered)
f1_engineered = f1_score(y_val, (y_pred_engineered > 0.5).astype(int))


print(f"Original features: AUC={auc_original:.4f}, F1={f1_original:.4f}")
print(f"Engineered features: AUC={auc_engineered:.4f}, F1={f1_engineered:.4f}")

"""## Model training

### Subtask:
Train several classification models on the training data and evaluate their performance on the validation set.

**Reasoning**:
Train several classification models and evaluate their performance on the validation set.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
}

# Train and evaluate models
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred_prob = model.predict_proba(X_val)[:, 1]
    y_pred = model.predict(X_val)
    auc = roc_auc_score(y_val, y_pred_prob)
    f1 = f1_score(y_val, y_pred)
    cm = confusion_matrix(y_val, y_pred)

    results[name] = {"AUC": auc, "F1": f1, "Confusion Matrix": cm}

    print(f"Model: {name}")
    print(f"AUC: {auc:.4f}")
    print(f"F1: {f1:.4f}")
    print("Confusion Matrix:")
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    print("-" * 20)

# Find best model (considering AUC and F1)
best_model = max(results, key=lambda k: results[k]["AUC"] * results[k]["F1"])
print(f"Best Model based on AUC*F1: {best_model}")